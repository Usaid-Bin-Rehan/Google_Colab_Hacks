{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "MapReduce.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFFDhQ0CXIkq"
      },
      "source": [
        "# **MapReduce**\n",
        "\n",
        "MapReduce is a programming model for processing and generating big data sets with a parallel and distributed algorithm on a single cluster. \n",
        "\n",
        "A MapReduce program is composed of a map procedure, which performs filtering and sorting, and a reduce method, which performs a summary operation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_uWYNdySZ6M"
      },
      "source": [
        "# Python's `map` function\n",
        "\n",
        "- Python has a built-in function `map` which is much faster than our version.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGIS2aVOSZ6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9720361d-e484-4d14-bb21-244165131ac4"
      },
      "source": [
        "map(lambda x: x*x, [1, 2, 3, 4, 5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7f17fe4d7c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUP5UuhySZ6Q"
      },
      "source": [
        "## Implementing reduce\n",
        "\n",
        "- The `reduce` function is an example of a [fold](https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29).\n",
        "\n",
        "- There are different ways we can fold data.\n",
        "\n",
        "- The following implements a *left* fold.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uStsM4nnSZ6R"
      },
      "source": [
        "def foldl(f, data, z):\n",
        "    if (len(data) == 0):\n",
        "        print (z)\n",
        "        return z\n",
        "    else:\n",
        "        head = data[0]\n",
        "        tail = data[1:]\n",
        "        print (\"Folding\", head, \"with\", tail, \"using\", z)\n",
        "        partial_result = f(z, data[0])\n",
        "        print (\"Partial result is\", partial_result)\n",
        "        return foldl(f, tail, partial_result)  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZPUX0SSZ6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb599c5b-7a84-4353-c298-022d13b7d56b"
      },
      "source": [
        "def add(x, y):\n",
        "    return x + y\n",
        "\n",
        "foldl(add, [3, 3, 3, 3, 3], 0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folding 3 with [3, 3, 3, 3] using 0\n",
            "Partial result is 3\n",
            "Folding 3 with [3, 3, 3] using 3\n",
            "Partial result is 6\n",
            "Folding 3 with [3, 3] using 6\n",
            "Partial result is 9\n",
            "Folding 3 with [3] using 9\n",
            "Partial result is 12\n",
            "Folding 3 with [] using 12\n",
            "Partial result is 15\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekYWQagiSZ6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a43d5c-407f-454c-c6ae-63d912a57830"
      },
      "source": [
        "foldl(lambda x, y: x + y, [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folding 1 with [2, 3, 4, 5] using 0\n",
            "Partial result is 1\n",
            "Folding 2 with [3, 4, 5] using 1\n",
            "Partial result is 3\n",
            "Folding 3 with [4, 5] using 3\n",
            "Partial result is 6\n",
            "Folding 4 with [5] using 6\n",
            "Partial result is 10\n",
            "Folding 5 with [] using 10\n",
            "Partial result is 15\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PSPmouGSZ6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea88249-aac5-4819-c0d1-16f66c014ed2"
      },
      "source": [
        "foldl(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folding 1 with [2, 3, 4, 5] using 0\n",
            "Partial result is -1\n",
            "Folding 2 with [3, 4, 5] using -1\n",
            "Partial result is -3\n",
            "Folding 3 with [4, 5] using -3\n",
            "Partial result is -6\n",
            "Folding 4 with [5] using -6\n",
            "Partial result is -10\n",
            "Folding 5 with [] using -10\n",
            "Partial result is -15\n",
            "-15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvLlfaXdSZ6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b0477a-5713-46f8-fb57-243f0102bec7"
      },
      "source": [
        "(((((0 - 1) - 2) - 3) - 4) - 5)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tGdnuooSZ6X"
      },
      "source": [
        "- Subtraction is neither [commutative](https://en.wikipedia.org/wiki/Commutative_property) nor [associative](https://en.wikipedia.org/wiki/Associative_property), so the order in which apply the fold matters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Aics6dTSZ6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12452616-0cd3-4ce9-deee-e52fdd5761b7"
      },
      "source": [
        "(1 - (2 - (3 - (4 - (5 - 0)))))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtfQCT3aSZ6Z"
      },
      "source": [
        "def foldr(f, data, z):\n",
        "    if (len(data) == 0):\n",
        "        return z\n",
        "    else:\n",
        "        return f(data[0], foldr(f, data[1:], z))                "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xqieCVjSZ6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08513367-41c8-45db-cb88-031425c57d31"
      },
      "source": [
        "foldl(lambda x, y: x - y,  [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folding 1 with [2, 3, 4, 5] using 0\n",
            "Partial result is -1\n",
            "Folding 2 with [3, 4, 5] using -1\n",
            "Partial result is -3\n",
            "Folding 3 with [4, 5] using -3\n",
            "Partial result is -6\n",
            "Folding 4 with [5] using -6\n",
            "Partial result is -10\n",
            "Folding 5 with [] using -10\n",
            "Partial result is -15\n",
            "-15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjNJrZWSZ6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bfcd8b-4644-4899-cdee-2a2645813d8f"
      },
      "source": [
        "foldr(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mnICxxvSZ6e"
      },
      "source": [
        "## Python's `reduce` function.\n",
        "\n",
        "- Python's built-in `reduce` function is a *left* fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfd2S_riSZ6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ac56df-4a91-4f2e-c460-1112568f1212"
      },
      "source": [
        "from functools import reduce\n",
        "reduce(lambda x, y: x + y, [1, 2, 3, 4, 5])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMebBUGSZ6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bfccba-52ca-4970-9f51-50daa0f71f70"
      },
      "source": [
        "reduce(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwGaPVmRSZ6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178a3262-706f-45a1-c98d-53d2efda31b4"
      },
      "source": [
        "foldl(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folding 1 with [2, 3, 4, 5] using 0\n",
            "Partial result is -1\n",
            "Folding 2 with [3, 4, 5] using -1\n",
            "Partial result is -3\n",
            "Folding 3 with [4, 5] using -3\n",
            "Partial result is -6\n",
            "Folding 4 with [5] using -6\n",
            "Partial result is -10\n",
            "Folding 5 with [] using -10\n",
            "Partial result is -15\n",
            "-15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgMBgnHdSZ6o"
      },
      "source": [
        "# Functional programming and parallelism\n",
        "\n",
        "- Functional programming lends itself to [parallel programming](https://computing.llnl.gov/tutorials/parallel_comp/#Models).\n",
        "\n",
        "- The `map` function can easily be parallelised through [data-level parallelism](https://en.wikipedia.org/wiki/Data_parallelism),\n",
        "    - provided that the function we supply as an argument is *free from* [side-effects](https://en.wikipedia.org/wiki/Side_effect_%28computer_science%29)\n",
        "        - (which is why we avoid working with mutable data).\n",
        "\n",
        "- We can see this by rewriting it so:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLOAd_LmSZ6p"
      },
      "source": [
        "def perform_computation(f, result, data, i):\n",
        "    print (\"Computing the \", i, \"th result...\")\n",
        "    # This could be scheduled on a different CPU\n",
        "    result[i] = f(data[i])\n",
        "\n",
        "def my_map(f, data):\n",
        "    result = [None] * len(data)\n",
        "    for i in range(len(data)):\n",
        "        perform_computation(f, result, data, i)\n",
        "    # Wait for other CPUs to finish, and then..\n",
        "    return result"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs7zcJqRSZ6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2c70f2-a212-4e39-b5aa-cb146166427d"
      },
      "source": [
        "my_map(lambda x: x * x, [1, 2, 3, 4, 5])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the  0 th result...\n",
            "Computing the  1 th result...\n",
            "Computing the  2 th result...\n",
            "Computing the  3 th result...\n",
            "Computing the  4 th result...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-n1XW7_SZ6r"
      },
      "source": [
        "## A multi-threaded `map` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSKvlVUbSZ6s"
      },
      "source": [
        "from threading import Thread\n",
        "\n",
        "def schedule_computation_threaded(f, result, data, threads, i):    \n",
        "    # Each function evaluation is scheduled on a different core.\n",
        "    def my_job(): \n",
        "        print (\"Processing data:\", data[i], \"... \")\n",
        "        result[i] = f(data[i])\n",
        "        print (\"Finished job #\", i)    \n",
        "        print (\"Result was\", result[i])       \n",
        "    threads[i] = Thread(target=my_job)\n",
        "    \n",
        "def my_map_multithreaded(f, data):\n",
        "    n = len(data)\n",
        "    result = [None] * n\n",
        "    threads = [None] * n\n",
        "    print (\"Scheduling jobs.. \")\n",
        "    for i in range(n):\n",
        "        schedule_computation_threaded(f, result, data, threads, i)\n",
        "    print (\"Starting jobs.. \")\n",
        "    for i in range(n):\n",
        "        threads[i].start()\n",
        "    print (\"Waiting for jobs to finish.. \")\n",
        "    for i in range(n):\n",
        "        threads[i].join()\n",
        "    print (\"All done.\")\n",
        "    return result"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etZk_tGNSZ6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740e5ff7-0c5d-4c05-87a3-18359939de7b"
      },
      "source": [
        "my_map_multithreaded(lambda x: x*x, [1, 2, 3, 4, 5])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scheduling jobs.. \n",
            "Starting jobs.. \n",
            "Processing data: 1 ... \n",
            "Finished job # 0\n",
            "Result was 1\n",
            "Processing data: 2 ... \n",
            "Finished job # 1\n",
            "Result was 4\n",
            "Processing data: 3 ... \n",
            "Finished job # 2\n",
            "Result was 9\n",
            "Processing data: 4 ... \n",
            "Finished job # 3\n",
            "Result was 16\n",
            "Processing data: 5 ... \n",
            "Finished job # 4\n",
            "Result was 25\n",
            "Waiting for jobs to finish.. \n",
            "All done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfaIUQCGSZ6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2d5bf7-d52a-4d88-d8af-45a866a217dc"
      },
      "source": [
        "from numpy.random import uniform\n",
        "from time import sleep\n",
        "\n",
        "def a_function_which_takes_a_long_time(x):\n",
        "    sleep(uniform(2, 10))  # Simulate some long computation\n",
        "    return x*x\n",
        "\n",
        "my_map_multithreaded(a_function_which_takes_a_long_time, [1, 2, 3, 4, 5])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scheduling jobs.. \n",
            "Starting jobs.. \n",
            "Processing data: 1 ... \n",
            "Processing data: 2 ... \n",
            "Processing data: 3 ... \n",
            "Processing data: 4 ... \n",
            "Processing data:Waiting for jobs to finish.. \n",
            " 5 ... \n",
            "Finished job # 4\n",
            "Result was 25\n",
            "Finished job # 0\n",
            "Result was 1\n",
            "Finished job # 2\n",
            "Result was 9\n",
            "Finished job # 3\n",
            "Result was 16\n",
            "Finished job # 1\n",
            "Result was 4\n",
            "All done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjj5_c_DSZ6v"
      },
      "source": [
        "## Map Reduce\n",
        "\n",
        "- Map Reduce is a _programming model_ for scalable parallel processing.\n",
        "- Scalable here means that it can work on big data with very large compute clusters.\n",
        "- There are many implementations: e.g. Apache Hadoop and Apache Spark.\n",
        "- We can use Map-Reduce with any programming language:\n",
        "    - Hadoop is written in Java\n",
        "    - Spark is written in Scala, but has a Python interface.\n",
        "- *Functional programming* languages such as Python or Scala fit very well with the Map Reduce model:\n",
        "    - However, we don't *have* to use functional programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgb_MHGnSZ6w"
      },
      "source": [
        "- A MapReduce implementation will take care of the low-level functionality so that you don't have to worry about:\n",
        "    - load balancing\n",
        "    - network I/O\n",
        "    - network and disk transfer optimisation\n",
        "    - handling of machine failures\n",
        "    - serialization of data\n",
        "    - etc..\n",
        "- The model is designed to move the processing to where the data resides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-85gkFRSZ6x"
      },
      "source": [
        "## Typical steps in a Map Reduce Computation\n",
        "\n",
        "1. ETL a big data set.\n",
        "2. _Map_ operation: extract something you care about from each row\n",
        "3. \"Shuffle and Sort\": task/node allocation\n",
        "4. _Reduce_ operation: aggregate, summarise, filter or transform\n",
        "5. Write the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccvzvAlRSZ6y"
      },
      "source": [
        "## Callbacks for Map Reduce\n",
        "\n",
        "- The data set, and the state of each stage of the computation, is represented as a set of key-value pairs.\n",
        "\n",
        "- The programmer provides a map function:\n",
        "\n",
        "$\\operatorname{map}(k, v) \\rightarrow \\; \\left< k', v' \\right>*$  \n",
        "\n",
        "- and a reduce function:\n",
        "\n",
        "$\\operatorname{reduce}(k', \\left< k', v'\\right> *) \\rightarrow \\; \\left< k', v''\n",
        "\\right> *$\n",
        "\n",
        "- The $*$ refers to a *collection* of values.\n",
        "\n",
        "- These collections are *not* ordered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5kGpBycSZ6z"
      },
      "source": [
        "## Word Count Example\n",
        "\n",
        "- In this simple example, the input is a set of URLs, each record is a document.\n",
        "\n",
        "- Problem: compute how many times each word has occurred across data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBVS1pMnSZ6z"
      },
      "source": [
        "## Word Count: Map \n",
        "\n",
        "\n",
        "- The input to $\\operatorname{map}$ is a mapping:\n",
        "\n",
        "- Key: URL\n",
        "- Value: Contents of document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj9wnqLvSZ60"
      },
      "source": [
        "$\\left< document1, to \\; be \\; or \\; not \\; to \\; be \\right>$  \n",
        "    \n",
        "\n",
        "- In this example, our $\\operatorname{map}$ function will process a given URL, and produces a mapping:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyQSHI-wSZ60"
      },
      "source": [
        "- Key: word\n",
        "- Value: 1\n",
        "\n",
        "- So our original data-set will be transformed to:\n",
        "  \n",
        "  $\\left< to, 1 \\right>$\n",
        "  $\\left< be, 1 \\right>$\n",
        "  $\\left< or, 1 \\right>$\n",
        "  $\\left< not, 1 \\right>$\n",
        "  $\\left< to, 1 \\right>$\n",
        "  $\\left< be, 1 \\right>$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q15yCUqSSZ61"
      },
      "source": [
        "## Word Count: Reduce\n",
        "\n",
        "\n",
        "- The reduce operation groups values according to their key, and then performs areduce on each key.\n",
        "\n",
        "- The collections are partitioned across different storage units, therefore.\n",
        "\n",
        "- Map-Reduce will fold the data in such a way that it minimises data-copying across the cluster.\n",
        "\n",
        "- Data in different partitions are reduced separately in parallel.\n",
        "\n",
        "- The final result is a reduce of the reduced data in each partition.\n",
        "\n",
        "- Therefore it is very important that our operator *is both commutative and associative*.\n",
        "\n",
        "- In our case the function is the `+` operator\n",
        "\n",
        "  $\\left< be, 2 \\right>$  \n",
        "  $\\left< not, 1 \\right>$  \n",
        "  $\\left< or, 1 \\right>$  \n",
        "  $\\left< to, 2 \\right>$  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMh-ecuSZ63"
      },
      "source": [
        "## Map and Reduce compared with Python\n",
        "\n",
        "- Notice that these functions are formulated differently from the standard Python functions of the same name.\n",
        "\n",
        "- The `reduce` function works with key-value *pairs*.\n",
        "\n",
        "- It would be more apt to call it something like `reduceByKey`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si55VDS4SZ64"
      },
      "source": [
        "## MiniMapReduce\n",
        "\n",
        "- To illustrate how the Map-Reduce programming model works, we can implement our own Map-Reduce framework in Python.\n",
        "\n",
        "- This *illustrates* how a problem can be written in terms of `map` and `reduce` operations.\n",
        "\n",
        "- Note that these are illustrative functions; this is *not* how Hadoop or Apache Spark actually implement them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjeqv4fqSZ64"
      },
      "source": [
        "##########################################################\n",
        "#\n",
        "#   MiniMapReduce\n",
        "#\n",
        "# A non-parallel, non-scalable Map-Reduce implementation\n",
        "##########################################################\n",
        "\n",
        "def groupByKey(data):\n",
        "    result = dict()\n",
        "    for key, value in data:\n",
        "        if key in result:\n",
        "            result[key].append(value)\n",
        "        else:\n",
        "            result[key] = [value]\n",
        "    return result\n",
        "        \n",
        "def reduceByKey(f, data):\n",
        "    key_values = groupByKey(data)\n",
        "    return map(lambda key: \n",
        "                   (key, reduce(f, key_values[key])), \n",
        "                       key_values)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqzXANmqSZ65"
      },
      "source": [
        "## Word-count using MiniMapReduce\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtPKb9wSZ67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3ebb53-e1a2-4a97-e01f-0ee9252fd616"
      },
      "source": [
        "data = map(lambda x: (x, 1), \"to be or not to be\".split())\n",
        "data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7f17fe4be090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKYV8XRMSZ69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29f58a5-48b5-40b7-da57-118023c1d97d"
      },
      "source": [
        "groupByKey(data)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'be': [1, 1], 'not': [1], 'or': [1], 'to': [1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3vMGCrfSZ6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df092c1-19d8-4003-86cc-4a0e4d0efc5a"
      },
      "source": [
        "reduceByKey(lambda x, y: x + y, data)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7f17fe48a850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph9NTRgfSZ6_"
      },
      "source": [
        "## Parallelising MiniMapReduce\n",
        "\n",
        "- We can easily turn our Map-Reduce implementation into a parallel, multi-threaded framework\n",
        "by using the `my_map_multithreaded` function we defined earlier.\n",
        "\n",
        "- This will allow us to perform map-reduce computations that exploit parallel processing using *multiple* cores on a *single* computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5oc2u_GSZ7A"
      },
      "source": [
        "def reduceByKey_multithreaded(f, data):\n",
        "    key_values = groupByKey(data)\n",
        "    return my_map_multithreaded(\n",
        "        lambda key: (key, reduce(f, key_values[key])), key_values.keys())"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD0E4B65SZ7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1159c658-66ca-4fad-8258-2b46131fb916"
      },
      "source": [
        "reduceByKey_multithreaded(lambda x, y: x + y, data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scheduling jobs.. \n",
            "Starting jobs.. \n",
            "Waiting for jobs to finish.. \n",
            "All done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beZYSkWLSZ7B"
      },
      "source": [
        "## Parallelising the reduce step\n",
        "\n",
        "- Provided that our operator is both associative and commutative we can\n",
        "also parallelise the reduce operation.\n",
        "\n",
        "- We partition the data into approximately equal subsets.\n",
        "\n",
        "- We then reduce each subset independently on a separate core.\n",
        "\n",
        "- The results can be combined in a final reduce step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUuQovQYSZ7B"
      },
      "source": [
        "### Partitioning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql2CooisSZ7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cd928b-70ca-40db-c91d-f52106aa9a67"
      },
      "source": [
        "def split_data(data, split_points):\n",
        "    partitions = []\n",
        "    n = 0\n",
        "    for i in split_points:\n",
        "        partitions.append(data[n:i])\n",
        "        n = i\n",
        "    partitions.append(data[n:])\n",
        "    return partitions\n",
        "\n",
        "data = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n",
        "partitioned_data = split_data(data, [3])\n",
        "partitioned_data"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a', 'b', 'c'], ['d', 'e', 'f', 'g']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfWC7c_sSZ7D"
      },
      "source": [
        "### Reducing across partitions in parallel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61DBHvUTSZ7E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3607c08b-24d3-4492-81e1-d4669fac1fcf"
      },
      "source": [
        "from threading import Thread\n",
        "\n",
        "def parallel_reduce(f, partitions):\n",
        "\n",
        "    n = len(partitions)\n",
        "    results = [None] * n\n",
        "    threads = [None] * n\n",
        "    \n",
        "    def job(i):\n",
        "        results[i] = reduce(f, partitions[i])\n",
        "\n",
        "    for i in range(n):\n",
        "        threads[i] = Thread(target = lambda: job(i))\n",
        "        threads[i].start()\n",
        "    \n",
        "    for i in range(n):\n",
        "        threads[i].join()\n",
        "    \n",
        "    return reduce(f, results)\n",
        "\n",
        "parallel_reduce(lambda x, y: x + y, partitioned_data)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'abcdefg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKlOnb0YSZ5J"
      },
      "source": [
        "Adopted from work by Steve Phelps:\n",
        "https://github.com/phelps-sg/python-bigdata \n",
        "\n",
        "Licensed under the Creative Commons Attribution 4.0 International license agreement.\n",
        "\n",
        "Indication of changes made: Eliminated cells didn't serve purpose of my use"
      ]
    }
  ]
}